{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Problem Statement and Modelling Objective**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling Objective: Create ML models that can use meterological and/or soil data to predict drought incidents\n",
    "\n",
    "Datasets: https://www.kaggle.com/datasets/cdminix/us-drought-meteorological-data?datasetId=1108326&sortBy=voteCount (rows and columns after dropping nulls and merging soil dataset: 678039 X 55)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Required Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "from pandas import option_context\n",
    "import joblib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading the Datasets and Merging**\n",
    "* Could not train models using the whole dataset due to memory errors, therefore to preserve memory-\n",
    "    * Changed the float64 to float32 \n",
    "    * Changed the date column to pd datetime\n",
    "    * Used 70% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = r'drought_data\\train_timeseries\\train_timeseries.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'fips': 'int16',\n",
    "    'PRECTOT': 'float32',\n",
    "    'PS': 'float32',\n",
    "    'QV2M': 'float32',\n",
    "    'T2M': 'float32',\n",
    "    'T2MDEW': 'float32',\n",
    "    'T2MWET': 'float32',\n",
    "    'T2M_MAX': 'float32',\n",
    "    'T2M_MIN': 'float32',\n",
    "    'T2M_RANGE': 'float32',\n",
    "    'TS': 'float32',\n",
    "    'WS10M': 'float32',\n",
    "    'WS10M_MAX': 'float32',\n",
    "    'WS10M_MIN': 'float32',\n",
    "    'WS10M_RANGE': 'float32',\n",
    "    'WS50M': 'float32',\n",
    "    'WS50M_MAX': 'float32',\n",
    "    'WS50M_MIN': 'float32',\n",
    "    'WS50M_RANGE': 'float32',\n",
    "    'score': 'float32'\n",
    "}\n",
    "\n",
    "sample_fraction = 0.7\n",
    "random.seed(45)\n",
    "\n",
    "def skip_row(row_idx):\n",
    "  if row_idx == 0:\n",
    "    return False\n",
    "  return random.random() > sample_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df = pd.read_csv(\n",
    "    r'drought_data\\train_timeseries\\train_timeseries.csv',\n",
    "    parse_dates= ['date'],\n",
    "    dtype=dtypes,\n",
    "    skiprows=skip_row \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_df = pd.read_csv(r'drought_data\\soil_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drought scores are available weekly while the daily meteorological data points are given. Therfore removing the rows where score (target column) is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df = drought_df.dropna()\n",
    "drought_df['score'] = drought_df['score'].round().astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the soil data with the drought dataset using 'fips' column present in both datasets which contains unique id for each US county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df_soil = drought_df.merge(\n",
    "    soil_df, \n",
    "    left_on='fips', \n",
    "    right_on='fips'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df_soil.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df_soil.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distribution of Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(drought_df_soil, x='score')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at correlation between different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corr = drought_df_soil.select_dtypes(np.number).corr()\n",
    "corr_sub = all_corr.sort_values(by='score', ascending=False).head(25)\n",
    "sns.heatmap(corr_sub[list(corr_sub.index)], annot=False, cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sub = all_corr.sort_values(by='score', ascending=False).head(15)\n",
    "sns.heatmap(corr_sub[list(corr_sub.index)], annot=False, cmap='Reds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "drought_df_soil.iloc[:, 1:10].boxplot()       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "drought_df_soil.iloc[:, 10:20].boxplot()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "drought_df_soil.iloc[:, 20:30].boxplot()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "drought_df_soil.iloc[:, 30:40].boxplot()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "drought_df_soil.iloc[:, 40:-3].boxplot()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(corr_sub.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(drought_df_soil.sample(n=1000), x='TS', y='T2M_MAX', color='score', opacity=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(drought_df_soil.loc[drought_df_soil.score != 0].sample(n=1000), x='T2M', y='T2M_MAX', color='score', opacity=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(drought_df_soil.loc[drought_df_soil.score != 0], x= 'GRS_LAND', color='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df_soil.GRS_LAND.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(drought_df_soil.loc[(drought_df_soil.score != 0) & (drought_df_soil.elevation < 500)], x= 'elevation', color='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(drought_df_soil.loc[(drought_df_soil.PRECTOT < 5) & (drought_df_soil.PRECTOT > 0.2)], x= 'PRECTOT', color='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(drought_df_soil.loc[drought_df_soil.score != 0], x= 'slope1', color='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df_soil.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(drought_df_soil.sample(n=1000), x= 'WS10M', y='WS50M', color='score', opacity=0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Engineering**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In EDA, we saw that the columns have a lot of outliers that may affect the accuracy. Using standard deviation and mean to identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df_soil.score.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df_soil.score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_ex_fipsNscore = list(drought_df.loc[:, ~drought_df.columns.isin(['score', 'fips', 'date'])].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_ex_fipsNscore_soil = list(drought_df_soil.loc[:, ~drought_df_soil.columns.isin(['score', 'fips', 'date'])].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, columns, n_std=3):\n",
    "    for col in columns:\n",
    "        # print('---------------------------------------')\n",
    "        # print(f'Removing outliers from column: {col}')\n",
    "\n",
    "        mean = df[col].mean()\n",
    "        sd = df[col].std()\n",
    "\n",
    "        df = df[(df[col] <= mean + (n_std * sd))]\n",
    "        df = df[(df[col] >= mean - (n_std * sd))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df = remove_outliers(drought_df, cols_ex_fipsNscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df_soil = remove_outliers(drought_df_soil, columns= cols_ex_fipsNscore_soil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df_soil.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drought is time/season dependent, I am also going to use day, month and year to train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df['year'] = drought_df.date.dt.year\n",
    "drought_df['month'] = drought_df.date.dt.month\n",
    "drought_df['day'] = drought_df.date.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df_soil['year'] = drought_df_soil.date.dt.year\n",
    "drought_df_soil['month'] = drought_df_soil.date.dt.month\n",
    "drought_df_soil['day'] = drought_df_soil.date.dt.day"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to train models only using meterological data first and then with both metereological and soil data to compare the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs = drought_df[cols_ex_fipsNscore + ['year', 'month', 'day']]\n",
    "x_inputs[cols_ex_fipsNscore + ['year', 'month', 'day']] = scaler.fit_transform(x_inputs[cols_ex_fipsNscore + ['year', 'month', 'day']])\n",
    "target = drought_df['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs_soil = drought_df_soil[cols_ex_fipsNscore_soil+['year', 'month', 'day']]\n",
    "x_inputs_soil[cols_ex_fipsNscore_soil+['year', 'month', 'day']] = scaler.fit_transform(x_inputs_soil[cols_ex_fipsNscore_soil+['year', 'month', 'day']])\n",
    "target_soil = drought_df_soil['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs_soil.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn.feature_selection.RFE to identify the identify the 24 most important features/columns for predicting drought scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_model = RandomForestClassifier(n_estimators=10)\n",
    "rfe = RFE(rfe_model, n_features_to_select=24)\n",
    "rfe.fit(x_inputs_soil, target_soil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(list(x_inputs_soil.columns), columns=['features'])\n",
    "feat_selection_df = pd.DataFrame(rfe.support_, columns=['selection'])\n",
    "feat_selection_df_merge = pd.merge(feature_df, feat_selection_df, left_index=True, right_index=True)\n",
    "print(feat_selection_df_merge.sort_values('selection', ascending=False).head(24))\n",
    "print(list(feat_selection_df_merge.sort_values('selection', ascending=False).head(24).features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_select = [\n",
    "    'PRECTOT',\n",
    "    'WS50M_MIN',\n",
    "    'month',\n",
    "    'WS10M_MAX',\n",
    "    'T2MWET',\n",
    "    'PS',\n",
    "    'QV2M',\t\n",
    "    'T2M',\t\n",
    "    'T2MDEW',\t\n",
    "    'T2M_MAX',\t\n",
    "    'T2M_MIN',\t\n",
    "    'T2M_RANGE',\t\n",
    "    'TS',\t\n",
    "    'WS10M',\t\n",
    "    'WS10M_RANGE',\t\n",
    "    'WS50M',\t\n",
    "    'WS50M_MAX',\t\n",
    "    'WS50M_RANGE',\n",
    "    'year',\n",
    "    'day'\n",
    "    ]\n",
    "# col_select_soil = ['PRECTOT',\n",
    "#  'WS10M_RANGE',\n",
    "#  'PS',\n",
    "#  'slope1',\n",
    "#  'elevation',\n",
    "#  'lon',\n",
    "#  'lat',\n",
    "#  'day',\n",
    "#  'year',\n",
    "#  'WS50M_RANGE',\n",
    "#  'WS50M_MIN',\n",
    "#  'WS50M_MAX',\n",
    "#  'WS50M',\n",
    "#  'month',\n",
    "#  'T2M_MAX',\n",
    "#  'QV2M',\n",
    "#  'T2M',\n",
    "#  'T2MDEW',\n",
    "#  'T2MWET',\n",
    "#  'GRS_LAND',\n",
    "#  'T2M_MIN',\n",
    "#  'T2M_RANGE',\n",
    "#  'TS',\n",
    "#  'WS10M']\n",
    "\n",
    "col_select_soil= [\n",
    "    'PRECTOT', 'WS10M_RANGE', 'month', 'year', \n",
    "    'GRS_LAND', 'PS', 'elevation', 'lon', 'lat', \n",
    "    'WS50M_RANGE', 'WS50M_MIN', 'WS50M_MAX', 'WS50M', \n",
    "    'day', 'WS10M', 'TS', 'T2M_RANGE', 'T2M_MIN', 'T2M_MAX', \n",
    "    'QV2M', 'T2M', 'WS10M_MAX', 'T2MDEW', 'T2MWET']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data for traing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "                                    x_inputs[col_select], \n",
    "                                    target, \n",
    "                                    test_size=0.2, \n",
    "                                    random_state=45\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_soil, x_test_soil, y_train_soil, y_test_soil = train_test_split(\n",
    "                                    x_inputs_soil[col_select_soil], \n",
    "                                    target_soil, \n",
    "                                    test_size=0.2, \n",
    "                                    random_state=45\n",
    "                                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In EDA, we saw that there is class imbalance in the dataset, i.e. we have a lot of rows where the drought score (target) is 0. Therefore, I am using imblearn.over_sampling.SMOTE to upsample the rows where the drought score is 1, 2, 3, 4 or 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_smote, y_train_smote = sm.fit_resample(x_train, y_train.ravel())\n",
    "x_test_smote, y_test_smote = sm.fit_resample(x_test, y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_smote.shape, y_train_smote.shape, x_test_smote.shape, y_test_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_soil_smote, y_train_soil_smote = sm.fit_resample(x_train_soil, y_train_soil.ravel())\n",
    "x_test_soil_smote, y_test_soil_smote = sm.fit_resample(x_test_soil, y_test_soil.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_soil_smote.shape, y_train_soil_smote.shape, x_test_soil_smote.shape, y_test_soil_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = {x for x in list(x_train_soil_smote.columns) if list(x_train_soil_smote.columns).count(x) > 1}\n",
    "print(dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_soil_smote.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelling, Predicting and Hyperparameter Tuning**\n",
    "\n",
    "* Decision Tree\n",
    "* Random Forest Classifier\n",
    "* Gradient Boosting\n",
    "\n",
    "I will train models with original data i.e., without upsampling, after upsampling, with meterological data and finally with metreological + soil data and observe the results. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Model #1: Decision Trees*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_test_params(x_tr, y_tr, x_tes, y_tes, **params):\n",
    "    model = DecisionTreeClassifier(random_state=45, **params).fit(x_tr, y_tr)\n",
    "    test_preds = model.predict(x_tes)\n",
    "    train_preds = model.predict(x_tr)\n",
    "\n",
    "    train_acc = accuracy_score(y_tr, train_preds)\n",
    "    test_acc = accuracy_score(y_tes, test_preds)\n",
    "    test_kappa = cohen_kappa_score(y_tes, test_preds)\n",
    "\n",
    "    cf = confusion_matrix(y_tes, test_preds, normalize='true')\n",
    "    plt.figure()\n",
    "    sns.heatmap(cf, annot=True, cmap= 'Greens')\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.xlabel('Target')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    \n",
    "    return train_acc, test_acc, test_kappa, model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in [20, 50, 100, 150]:\n",
    "    print(f\"max_depth: {i}\")\n",
    "    acc.append(dt_test_params(\n",
    "        x_train, \n",
    "        y_train,\n",
    "        x_test,\n",
    "        y_test, \n",
    "        max_depth = i\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = pd.DataFrame(acc, columns=['train_acc', 'test_acc', 'kappa', 'model'])\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Overfitting curve for selected parameters')\n",
    "plt.plot([20, 50, 100, 150], 1- results1.train_acc, 'b-o')\n",
    "plt.plot([20, 50, 100, 150], 1- results1.test_acc, 'g-o')\n",
    "#plt.plot(50, acc[1], 'r-o')\n",
    "plt.xlabel('Hyperparameter Values')\n",
    "plt.ylabel('Error')\n",
    "plt.legend(['Training Error', 'Test Error'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using balanced dataset computed using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = []\n",
    "for i in [20, 50, 100, 150]:\n",
    "    print(f\"max_depth: {i}\")\n",
    "    acc1.append(dt_test_params(\n",
    "        x_train_smote, \n",
    "        y_train_smote,\n",
    "        x_test,\n",
    "        y_test, \n",
    "        max_depth = i\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = pd.DataFrame(acc1, columns=['train_acc', 'test_acc', 'kappa', 'model'])\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Overfitting curve for selected parameters')\n",
    "plt.plot([20, 50, 100, 150], 1- results2.train_acc, 'b-o')\n",
    "plt.plot([20, 50, 100, 150], 1- results2.test_acc, 'g-o')\n",
    "#plt.plot(50, acc[1], 'r-o')\n",
    "plt.xlabel('Hyperparameter Values')\n",
    "plt.ylabel('Error')\n",
    "plt.legend(['Training Error', 'Test Error'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using balanced dataset which includes both metereological and soil dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc2 = []\n",
    "for i in [25, 50, 75, 100]:\n",
    "    print(f\"max_depth: {i}\")\n",
    "    acc2.append(dt_test_params(\n",
    "        x_tr = x_train_soil_smote, \n",
    "        y_tr = y_train_soil_smote,\n",
    "        x_tes = x_test_soil,\n",
    "        y_tes = y_test_soil, \n",
    "        max_depth = i\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = pd.DataFrame(acc2, columns=['train_acc', 'test_acc', 'kappa', 'model'])\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Overfitting curve for selected parameters')\n",
    "plt.plot([20, 50, 100, 150], 1- results3.train_acc, 'b-o')\n",
    "plt.plot([20, 50, 100, 150], 1- results3.test_acc, 'g-o')\n",
    "#plt.plot(50, acc[1], 'r-o')\n",
    "plt.xlabel('Hyperparameter Values')\n",
    "plt.ylabel('Error')\n",
    "plt.legend(['Training Error', 'Test Error'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertuning 3. Using balanced dataset which includes both metereological and soil dataset\n",
    "\n",
    "Using sklearn.model_selection.GridSearchCV to compare different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': [30, 50, 100],\n",
    "    'min_samples_leaf': [10, 30, 65, 100],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_dt = GridSearchCV(\n",
    "                        estimator=DecisionTreeClassifier(random_state=45),\n",
    "                        param_grid=params, \n",
    "                        cv=4, \n",
    "                        n_jobs=-1, \n",
    "                        verbose=1, \n",
    "                        scoring = \"accuracy\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_dt.fit(pd.DataFrame(x_train_soil_smote).sample(frac=0.5, random_state=45), pd.DataFrame(y_train_soil_smote).sample(frac=0.5, random_state=45)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_dt = pd.DataFrame(grid_search_dt.cv_results_)\n",
    "with option_context('display.max_colwidth', None):\n",
    "    # display the dataframe\n",
    "    display(score_df_dt.sort_values('mean_test_score', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_test_params(\n",
    "        x_tr = x_train_soil_smote, \n",
    "        y_tr = y_train_soil_smote,\n",
    "        x_tes = x_test_soil,\n",
    "        y_tes = y_test_soil, \n",
    "        max_depth =  100, \n",
    "        max_features= None, \n",
    "        min_samples_leaf= 10\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Model #2: Random Forest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_test_params(x_tr, y_tr, x_tes, y_tes, **params):\n",
    "    model = RandomForestClassifier(random_state=45, **params).fit(x_tr, y_tr)\n",
    "    test_preds = model.predict(x_tes)\n",
    "\n",
    "    #train_acc = accuracy_score(y_train_smote, train_preds)\n",
    "    test_acc = accuracy_score(y_tes, test_preds)\n",
    "    test_kappa = cohen_kappa_score(y_tes, test_preds)\n",
    "\n",
    "    cf = confusion_matrix(y_tes, test_preds, normalize='true')\n",
    "    plt.figure()\n",
    "    sns.heatmap(cf, annot=True, cmap= 'Greens')\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.xlabel('Target')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    \n",
    "    return [test_acc, test_kappa, model]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random Forest model with metereological and soil data upsampled (Not going to try data with imbalanced classes since the results from balanced datasets were better with decision trees classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_params(\n",
    "    x_train_soil_smote, \n",
    "    y_train_soil_smote,\n",
    "    x_test_soil,\n",
    "    y_test_soil, \n",
    "    max_depth = 50,\n",
    "    n_estimators = 100\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning 1. Random Forest model with metereological and soil data upsampled\n",
    "\n",
    "Using sklearn.model_selection.RandomizedSearchCV to find appropriate hyperparameters (Tried GridSearchCV first but was it was taking to long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "    'n_estimators': [20, 50, 100, 150],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': [10, 50, 100, 150],\n",
    "    'bootstrap': [True, False]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search_rf = RandomizedSearchCV(\n",
    "                        estimator=RandomForestClassifier(random_state=45, n_estimators=50),\n",
    "                        param_distributions=params_rf,\n",
    "                        n_iter=25, \n",
    "                        cv=3, \n",
    "                        n_jobs=-1, \n",
    "                        verbose=20, \n",
    "                        random_state=45\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train_soil_smote).sample(frac=0.05, random_state=45).to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search_rf.fit(\n",
    "    pd.DataFrame(x_train_soil_smote).sample(frac=0.05, random_state=45), \n",
    "    pd.DataFrame(y_train_soil_smote).sample(frac=0.05, random_state=45).to_numpy().ravel()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search_rf.best_params_\n",
    "# score_df_rf = pd.DataFrame(rand_search_rf.cv_results_)\n",
    "# with option_context('display.max_colwidth', None):\n",
    "#     display(score_df_rf.sort_values('mean_test_score', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_params(\n",
    "    x_train_soil_smote, \n",
    "    y_train_soil_smote,\n",
    "    x_test_soil,\n",
    "    y_test_soil, \n",
    "    n_estimators = 50,\n",
    "    max_features= None,\n",
    "    max_depth = 100,\n",
    "    bootstrap= True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Model #3: XGBoost*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_test_params(x_tr, y_tr, x_tes, y_tes, **params):\n",
    "    model = XGBClassifier(random_state=45, n_jobs=-1, **params).fit(x_tr, y_tr)\n",
    "    test_preds = model.predict(x_tes)\n",
    "\n",
    "    #train_acc = accuracy_score(y_train_smote, train_preds)\n",
    "    test_acc = accuracy_score(y_tes, test_preds)\n",
    "    test_kappa = cohen_kappa_score(y_tes, test_preds)\n",
    "\n",
    "    cf = confusion_matrix(y_tes, test_preds, normalize='true')\n",
    "    plt.figure()\n",
    "    sns.heatmap(cf, annot=True, cmap= 'Greens')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.xlabel('Target')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    \n",
    "    return test_acc, test_kappa, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_params(\n",
    "    pd.DataFrame(x_train_soil_smote).sample(frac=0.4, random_state=45), \n",
    "    pd.DataFrame(y_train_soil_smote).sample(frac=0.4, random_state=45),\n",
    "    x_test_soil,\n",
    "    y_test_soil, \n",
    "    n_estimators = 100,\n",
    "    max_depth = 50,\n",
    "    learning_rate = 0.9,\n",
    "    verbosity = 1,\n",
    "    objective='multi:softprob',\n",
    "    subsample = 0.8,\n",
    "    colsample_bytree = 0.8\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [10, 25, 50, 100],\n",
    "    'learning_rate': [0.1, 0.3, 0.7, 0.9],\n",
    "    'subsample': [0.5, 0.8],\n",
    "    'colsample_bytree': [0.5, 0.8]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search_xgb = RandomizedSearchCV(\n",
    "                        estimator=XGBClassifier(random_state=45, objective = 'multi:softprob'),\n",
    "                        param_distributions=params_xgb,\n",
    "                        n_iter=25, \n",
    "                        cv=3, \n",
    "                        n_jobs=-1, \n",
    "                        verbose=2, \n",
    "                        random_state=45\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search_xgb.fit(\n",
    "    pd.DataFrame(x_train_soil_smote).sample(frac=0.05, random_state=45), \n",
    "    pd.DataFrame(y_train_soil_smote).sample(frac=0.05, random_state=45)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_params(\n",
    "    pd.DataFrame(x_train_soil_smote).sample(frac=0.4, random_state=45), \n",
    "    pd.DataFrame(y_train_soil_smote).sample(frac=0.4, random_state=45),\n",
    "    x_test_soil,\n",
    "    y_test_soil, \n",
    "    n_estimators = 150,\n",
    "    max_depth = 25,\n",
    "    learning_rate = 0.1,\n",
    "    verbosity = 1,\n",
    "    objective='multi:softprob',\n",
    "    subsample = 0.8,\n",
    "    colsample_bytree = 0.8\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Performance of the Best Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifiers</th>\n",
       "      <th>Hyperparameters Used</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Kappa Score</th>\n",
       "      <th>Training and Prediction Time (min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>n_estimators: 50, max_features: None, max_depth: 100, bootstrap: True</td>\n",
       "      <td>0.8902</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>n_estimators: 150, max_depth: 25, learning_rate: 0.1, objective: multi:softprob, subsample: 0.8, colsample_bytree: 0.8</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.7842</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Trees</td>\n",
       "      <td>max_depth: 100, max_features: None, min_samples_leaf: 10</td>\n",
       "      <td>0.8310</td>\n",
       "      <td>0.7108</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Classifiers  \\\n",
       "1      Random Forest   \n",
       "2  Gradient Boosting   \n",
       "0     Decision Trees   \n",
       "\n",
       "                                                                                                     Hyperparameters Used  \\\n",
       "1                                                   n_estimators: 50, max_features: None, max_depth: 100, bootstrap: True   \n",
       "2  n_estimators: 150, max_depth: 25, learning_rate: 0.1, objective: multi:softprob, subsample: 0.8, colsample_bytree: 0.8   \n",
       "0                                                                max_depth: 100, max_features: None, min_samples_leaf: 10   \n",
       "\n",
       "   Test Accuracy  Kappa Score  Training and Prediction Time (min)  \n",
       "1         0.8902       0.8082                                  87  \n",
       "2         0.8768       0.7842                                  21  \n",
       "0         0.8310       0.7108                                   2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_res = {\n",
    "    'Classifiers': ['Decision Trees', 'Random Forest', 'Gradient Boosting'],\n",
    "    'Hyperparameters Used': \n",
    "        [\n",
    "            'max_depth: 100, max_features: None, min_samples_leaf: 10',\n",
    "            'n_estimators: 50, max_features: None, max_depth: 100, bootstrap: True',\n",
    "            'n_estimators: 150, max_depth: 25, learning_rate: 0.1, objective: multi:softprob, subsample: 0.8, colsample_bytree: 0.8'\n",
    "        ],\n",
    "    'Test Accuracy': [0.8310, 0.8902, 0.8768],\n",
    "    'Kappa Score': [0.7108, 0.8082, 0.7842],\n",
    "    'Training and Prediction Time (min)': [2, 87, 21]\n",
    "}\n",
    "with option_context('display.max_colwidth', None):\n",
    "    display(pd.DataFrame(all_res).sort_values('Test Accuracy', ascending=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Confusion Matrix- Random Forest</p>\n",
    "<img src=\"confusion/rf.png\"/>\n",
    "\n",
    "<p>Confusion Matrix- Gradient Boosting</p>\n",
    "<img src=\"confusion/xgb.png\"/>\n",
    "\n",
    "<p>Confusion Matrix- Decicion Trees</p>\n",
    "<img src=\"confusion/dt.png\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Sample Predictions of Final Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samp_test = x_test_soil.sample(random_state=105, n=100).reset_index(drop=True)\n",
    "y_samp_test = pd.DataFrame(y_test_soil.sample(random_state=105, n=100)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model =   RandomForestClassifier(\n",
    "                n_jobs=-1,\n",
    "                random_state=45,\n",
    "                n_estimators = 50,\n",
    "                max_features= None,\n",
    "                max_depth = 100,\n",
    "                bootstrap= True\n",
    "            )\n",
    "final_model.fit(x_train_soil_smote, y_train_soil_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_predictions = final_model.predict(x_samp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_predictions = pd.DataFrame(sample_predictions, columns=['Model Predictions']).merge(\n",
    "    y_samp_test.rename(columns = {'score':'Target Scores'}), \n",
    "    left_index=True, \n",
    "    right_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_predictions.sort_values('Target Scores', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Samples: {len(y_samp_test)}\")\n",
    "print(f\"Total Correct Predictions: {accuracy_score(y_samp_test, sample_predictions['Model Predictions'], normalize = False)}\")\n",
    "print(f\"Total Incorrect Predictions: {len(y_samp_test) - accuracy_score(y_samp_test, sample_predictions['Model Predictions'], normalize = False)}\")\n",
    "print(f\"Sample Prediction Accuracy: {accuracy_score(y_samp_test, sample_predictions['Model Predictions'])}\")\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(y_samp_test, sample_predictions['Model Predictions'], normalize= None), annot=True, cmap= 'Greens')\n",
    "plt.ylabel('Predictions')\n",
    "plt.xlabel('Target')\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Saving the Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_converter(df, date_col_name):\n",
    "    if df.date_col_name.dtype != 'datetime64[ns]':\n",
    "        df['pd_datetime'] = pd.to_datetime(df.date_col_name)\n",
    "        df['year'] = df.pd_datetime.dt.year\n",
    "        df['month'] = df.pd_datetime.dt.month\n",
    "        df['day'] = df.pd_datetime.dt.day\n",
    "    else:\n",
    "        df['year'] = df.date_col_name.dt.year\n",
    "        df['month'] = df.date_col_name.dt.month\n",
    "        df['day'] = df.date_col_name.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_imputer(df):\n",
    "    ask = input('Want to drop all rows with nulls? (y/n)')\n",
    "    if ask == 'y':\n",
    "        df = df.dropna()\n",
    "    elif ask == 'n':\n",
    "        pass\n",
    "    else:\n",
    "        print('Error: Invalid Input, operation not completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soil_data_merge(df, soil_df, merge_col_left, merge_col_right):\n",
    "    df = drought_df.merge(\n",
    "    soil_df, \n",
    "    left_on=merge_col_left, \n",
    "    right_on=merge_col_right\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_prediction_RF_Model = {\n",
    "    'model': final_model,\n",
    "    'scaler': scaler,\n",
    "    'input_cols': col_select_soil,\n",
    "    'target_col': 'score',\n",
    "    'imputer': my_imputer,\n",
    "    'date_converter': date_converter,\n",
    "    'merge_soil_data': soil_data_merge\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(drought_prediction_RF_Model, 'drought_prediction_RF_Model.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Summary and Recommendations**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "<br>\n",
    "Random Forest classifier (RF) trained with upsampled metereological and soil data was able to produce higher accuracy than Decision Trees classifier (DT) and gradient boosting (XGB) (Note: XGB classifier was only trained with 40% of the data). Results of hypertuned RF classifier was significantly better than hypertuned DT classifier but it took almost 40 times more time than DT classifier to train and make predictions. Eventhough XGB classifier was trained with only 40% of the data,results produced by it and RF classifier were comparable.  \n",
    "<br>\n",
    "<br>\n",
    "Observations Regarding Data:\n",
    "* Observation 1: Dataset was imbalanced i.e. drought score of 0 was significantly higher in numbers than other scores.\n",
    "<br>\n",
    "<br>\n",
    "Treatment: Oversampling/upsampling was done to balance the dataset.\n",
    "<br>\n",
    "Treatment Result: Accuracy improved\n",
    "\n",
    "* Observation 2: Dataset had many outliers\n",
    "<br>\n",
    "<br>\n",
    "Treatment: Outliers were removed using standard deviation and mean\n",
    "<br>\n",
    "Treatment Result: Unknown (Did not train model using outliers)\n",
    "\n",
    "* Observation 3: Columns related to temperature and wind speed had high correlation.\n",
    "<br>\n",
    "<br>\n",
    "Treatment: None\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Suggestions/Recommendations/Questions for Future Work:\n",
    " \n",
    "* Was removing outliers a good idea? Did it affect the accuracy?\n",
    "* How would combining temerature and windspeed related columns (for eg. using PCA) affect the accuracy? Will that reduce the training time significantly?\n",
    "* If XGB classifier was trained with the whole dataset, would it produce better results than RF classifier.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **References**\n",
    "\n",
    "* Reference Notebook: https://www.kaggle.com/code/akshayasrinivasan2/drought-prediction-using-ml-algorithms\n",
    "* Feature Selection: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "* Feature Selection: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "* Dealing with Unbalanced Datasets/Oversampling: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "* XGBoost Reference: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "* XGBoost Hyperparameter Tuning: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "* Dealing with Outliers: https://medium.com/analytics-vidhya/how-to-remove-outliers-for-machine-learning-24620c4657e8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:53) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e31f4fc8e592552db8e24aef814b2f295d66babc5cfa7fcf9ba0f017b70584e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
